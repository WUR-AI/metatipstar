{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hXrYtDFXjcfk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXrYtDFXjcfk",
    "outputId": "5abcfe0f-2c69-4dab-ea11-a64cc182f3ae"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def setup():\n",
    "    colab = 'google.colab' in str(get_ipython())\n",
    "    if not colab:\n",
    "        repodir = '/home/michiel/WUR/metatipstar'\n",
    "        return repodir\n",
    "\n",
    "    !rm -fr /content/metatipstar\n",
    "    !git clone https://yourname:yourtoken@github.com/BigDataWUR/metatipstar.git\n",
    "    !pip install --quiet \"pytorch-lightning\" \"audtorch\"\n",
    "    repodir = '/content/metatipstar'\n",
    "    return repodir\n",
    "repodir = setup()\n",
    "csvdir = os.path.join(repodir, 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1",
   "metadata": {
    "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "scale_factor_target = 107.827207557798\n",
    "soilnr = [2010, 2040, 2050, 2060, 2070, 2080, 4010, 4020, 4031, \n",
    " 4040, 4041, 4050, 4070, 4090, 4130, 4140, 4160, 8060,\n",
    " 8090, 8101, 8110, 8120, 10010, 10030, 10061, 10080, \n",
    " 10190, 10191, 10240, 11030, 11040, 11050]\n",
    "dict_soilnr = {nr : i for i, nr in enumerate(soilnr)}\n",
    "\n",
    "feature_variables = ['irradiance','irrigation','mintemp','maxtemp','precipitation',\n",
    "                     'baseN','sidedressdoy','sidedressamount','sowdoy','maxRootDepthDueToSoil', 'Earliness',\n",
    "                     'soilprofile']\n",
    "\n",
    "target_variables = ['max_tuberfreshwt']\n",
    "timeseries = ['irradiance','irrigation','mintemp','maxtemp','precipitation']\n",
    "categorical = ['soilprofile']\n",
    "\n",
    "def polish_expid(x):\n",
    "    if (\"_\" in x):\n",
    "        expid=str(x).split('_')[0]\n",
    "        year=str(x).split('_')[-1][2:]\n",
    "        return f'PR{year}T{expid}'\n",
    "    return x\n",
    "    \n",
    "    number = ''.join([x for x in expid if x.isdigit()])\n",
    "    return number[0:2]\n",
    "\n",
    "class NitrogenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor_timeseries, tensor_continuous, tensor_categorical, tensor_targets, \n",
    "                 featurenames_timeseries, featurenames_continuous, featurenames_categorical, featurenames_targets):\n",
    "        assert tensor_timeseries.size(dim=1) == len(featurenames_timeseries)\n",
    "        assert tensor_continuous.size(dim=1) == len(featurenames_continuous)\n",
    "        assert tensor_categorical.size(dim=1) == len(featurenames_categorical)\n",
    "        assert tensor_targets.size(dim=1) == len(featurenames_targets)\n",
    "        self.tensor_timeseries = torch.nan_to_num(tensor_timeseries)\n",
    "        self.tensor_continuous = torch.nan_to_num(tensor_continuous)\n",
    "        self.tensor_categorical = torch.nan_to_num(tensor_categorical)\n",
    "        self.tensor_targets = torch.nan_to_num(tensor_targets)\n",
    "        self.featurenames_timeseries = featurenames_timeseries\n",
    "        self.featurenames_continuous = featurenames_continuous\n",
    "        self.featurenames_categorical = featurenames_categorical\n",
    "        self.featurenames_targets = featurenames_targets\n",
    "        #quick hack to correct flaws in 'sidedressdoy'\n",
    "        #self.tensor_continuous[:, self.featurenames_continuous.index('sidedressdoy')] = torch.clamp(self.tensor_continuous[:, self.featurenames_continuous.index('sidedressdoy')], min=0) \n",
    "        self.tensor_continuous[:, self.featurenames_continuous.index('sidedressdoy')] = 0.0\n",
    "        #self.tensor_continuous[:, self.featurenames_continuous.index('sowdoy')] = 0.0\n",
    "        self.experimentids = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_timeseries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensor_timeseries[index], self.tensor_continuous[index], self.tensor_categorical[index], self.tensor_targets[index], index\n",
    "\n",
    "\n",
    "def create_torch_dataset(num_examples, length_time_series, feature_variables, inputdir, tag):\n",
    "    print(f'create dataset {tag} {num_examples}')\n",
    "    set_timeseries = sorted(list(set(feature_variables) & set(timeseries)))\n",
    "    set_scalars = sorted(list(set(feature_variables) - set(set_timeseries)))\n",
    "    set_continuous = sorted(list(set(set_scalars) - set(categorical)))\n",
    "    tensor_timeseries = torch.zeros([num_examples, len(set_timeseries), length_time_series])\n",
    "    tensor_continuous = torch.zeros([num_examples, len(set_continuous)])\n",
    "    tensor_categorical = torch.zeros([num_examples, len(categorical)],dtype=int)\n",
    "    tensor_targets = torch.zeros([num_examples, len(target_variables)])\n",
    "    featurenames_timeseries = []\n",
    "    for i, name in enumerate(set_timeseries):\n",
    "        ds_path = os.path.join(inputdir, f'{name}-{tag}.csv.gz')\n",
    "        v_data = pd.read_csv(ds_path, sep=',')\n",
    "        print(f'load {ds_path} {v_data.shape}')\n",
    "        tensor_timeseries[:,i,:] = torch.tensor(v_data.values)\n",
    "        featurenames_timeseries.append(name)\n",
    "    ds_path = os.path.join(inputdir, f'si-{tag}.csv.gz')\n",
    "    scalar_data = pd.read_csv(ds_path, sep=',')\n",
    "    print(f'load {ds_path} {scalar_data.shape}')\n",
    "    set_continuous = scalar_data[set_continuous]\n",
    "    featurenames_continuous = list(set_continuous.columns)\n",
    "    tensor_continuous[:,:] = torch.tensor(set_continuous.values)\n",
    "\n",
    "    for i, c in enumerate(categorical):\n",
    "        df_nr = scalar_data.filter(regex=c).idxmax(axis=1).map(lambda x: int(str(x).split('_')[-1]))\n",
    "        df_nr = df_nr.map(lambda x : dict_soilnr[x])\n",
    "        tensor_categorical[:,i] = torch.tensor(df_nr.values.astype(int))\n",
    "   \n",
    "    ds_path = os.path.join(inputdir, f'response-{tag}.csv.gz')\n",
    "    target_data = pd.read_csv(ds_path, sep=',')\n",
    "    print(f'load {ds_path} {target_data.shape}')\n",
    "    target_data.columns = target_variables\n",
    "    target_data = target_data[target_variables]\n",
    "    featurenames_targets = list(target_data.columns)\n",
    "    tensor_targets[:,:] = torch.tensor(target_data.values)\n",
    "    dataset = NitrogenDataset(tensor_timeseries, tensor_continuous, tensor_categorical, tensor_targets, featurenames_timeseries, featurenames_continuous, categorical, featurenames_targets)\n",
    "    if 'exp' in scalar_data:  \n",
    "        dataset.experimentids = scalar_data['exp'].map(lambda x: polish_expid(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f",
    "outputId": "ee5dfa85-3aae-4985-a736-d0607fec48b4"
   },
   "outputs": [],
   "source": [
    "sets = ['train', 'val', 'test', 'obs']\n",
    "datasets = {}\n",
    "for s in sets:\n",
    "    ds_path = os.path.join(csvdir,f'irradiance-{s}.csv.gz')\n",
    "    num_examples, length_timeseries = pd.read_csv(ds_path, sep=',').shape\n",
    "    dataset = create_torch_dataset(num_examples, length_timeseries, feature_variables, inputdir=csvdir, tag=s)\n",
    "    datasets[s] = dataset\n",
    "    \n",
    "num_examples, length_timeseries = pd.read_csv(os.path.join(csvdir, 'irradiance-obs.csv.gz'), sep=',').shape\n",
    "dataset = create_torch_dataset(num_examples, length_timeseries, feature_variables, inputdir=csvdir, tag='obs')\n",
    "target_data = pd.read_csv(os.path.join(csvdir, 'tipstar-obs.csv.gz'), sep=',')\n",
    "target_data = target_data[['x']]\n",
    "dataset.tensor_targets[:,:] = torch.tensor(target_data.values)\n",
    "datasets['tipstar'] = dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab32c8c-7a05-4aa4-b180-053278a469db",
   "metadata": {
    "id": "3ab32c8c-7a05-4aa4-b180-053278a469db"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "r = range(21, 23, 1)\n",
    "fig, axes = plt.subplots(len(r), 1, sharex=True, figsize=(len(r)*4, len(r)*5))\n",
    "for i, j in enumerate(r):\n",
    "    ax = axes[i]\n",
    "    x=np.tile(range(datasets['obs'].tensor_timeseries.shape[2]),(5,1)).T\n",
    "    sample=j\n",
    "    y=datasets['obs'].tensor_timeseries[sample,:,:].T\n",
    "    ax.step(x, y, where='post',label=datasets['obs'].featurenames_timeseries)\n",
    "    ax.set_title(f\"{s} (i={sample} ({datasets['obs'].experimentids[sample]}) ({scale_factor_target*datasets['obs'].tensor_targets[sample,:].item():.2f}))\")\n",
    "    ax.set_ylim((0,1))\n",
    "fig.legend(datasets['obs'].featurenames_timeseries)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae306b1-cb1b-45d5-abdb-b35838a53b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "plot_sets = [s for s in datasets.keys() if s != \"tipstar\"]\n",
    "fig, axes = plt.subplots(len(plot_sets),1, sharex=True, figsize=(15,20)) #, subplot_kw=dict(box_aspect=1)\n",
    "for i,s in enumerate(plot_sets):\n",
    "    ax = axes[i]\n",
    "    x=np.tile(range(datasets[s].tensor_timeseries.shape[2]),(5,1)).T\n",
    "    sample=randrange(datasets[s].tensor_timeseries.shape[0])\n",
    "    y=datasets[s].tensor_timeseries[sample,:,:].T\n",
    "    ax.step(x, y, where='post',label=datasets[s].featurenames_timeseries)\n",
    "    ax.set_title(f'{s} (i={sample})')\n",
    "    ax.set_ylim((0,1))\n",
    "fig.legend(datasets['train'].featurenames_timeseries) #, bbox_to_anchor=(0.9,0.9), loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jOIb-HEW0BSh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jOIb-HEW0BSh",
    "outputId": "3c42ad4a-e5fe-4200-bf51-23ae8f5a1473"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "nvar = datasets['train'].tensor_timeseries.shape[1]\n",
    "nx = datasets['train'].tensor_timeseries.shape[2]\n",
    "fig, axes = plt.subplots(nvar, 1, sharex=True, figsize=(15,20))\n",
    "plot_sets = [s for s in datasets.keys() if s != \"tipstar\"]\n",
    "for v, v_name in enumerate(datasets['train'].featurenames_timeseries):  \n",
    "    ax = axes[v]\n",
    "    plot_data = np.zeros((nx, len(plot_sets)))\n",
    "    for i, s in enumerate(plot_sets):\n",
    "        y = datasets[s].tensor_timeseries[:,v,:].numpy().squeeze()\n",
    "        y = np.mean(y, axis=0).T\n",
    "        plot_data[:,i] = y\n",
    "    x = np.tile(range(nx), (len(plot_sets),1)).T\n",
    "    ax.step(x, plot_data, where='post', label=s, zorder=0)\n",
    "    ax.set_ylim((0,1))\n",
    "    ax.grid()\n",
    "    ax.set_title(f'{v_name}')\n",
    "fig.legend(plot_sets) #, bbox_to_anchor=(0.9,0.9), loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe",
    "outputId": "666d24bf-1c00-4efa-bfce-a869e1e57391"
   },
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = datasets['train'].featurenames_continuous + datasets['train'].featurenames_categorical + datasets['train'].featurenames_targets\n",
    "fig, axes = plt.subplots(len(features),1, figsize=(20,20))\n",
    "plot_sets = [s for s in datasets.keys() if s != \"tipstar\"]\n",
    " \n",
    "for i,featurename in enumerate(features):\n",
    "    print(f'{i}/{len(features)} feature: {featurename}')\n",
    "    ax_left=axes[i]\n",
    "    df_list = []\n",
    "    for s in plot_sets:\n",
    "        if featurename in categorical:\n",
    "            f = datasets['train'].featurenames_categorical.index(featurename)\n",
    "            y = datasets[s].tensor_categorical[:,f].detach()/len(dict_soilnr)\n",
    "        elif featurename in target_variables:\n",
    "            f = datasets['train'].featurenames_targets.index(featurename)\n",
    "            y = datasets[s].tensor_targets[:,f].detach()\n",
    "        else:\n",
    "            f = datasets['train'].featurenames_continuous.index(featurename)\n",
    "            y = datasets[s].tensor_continuous[:,f].detach()\n",
    "        df = pd.DataFrame(data=y, columns=[featurename])\n",
    "        df['dataset'] = s\n",
    "        df_list.append(df)\n",
    "    df_long = pd.concat(df_list, sort=False,ignore_index=True)\n",
    "    sns.histplot(data=df_long, x=featurename, hue='dataset', stat=\"probability\", common_norm=False, ax=ax_left, legend=False, multiple='dodge')\n",
    "    ax_left.set_xlabel('')\n",
    "    ax_left.set_title(f'{featurename}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477ebf3-0436-40af-a874-d3849cc87024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audtorch.metrics.functional import pearsonr\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def compute_metrics(targets, predictions):\n",
    "    metrics = {}\n",
    "    criterion = nn.MSELoss()\n",
    "    #torch.mul(predictions, scale_factor_target)\n",
    "    #torch.mul(targets, scale_factor_target)\n",
    "    metrics['rmse'] = round(torch.sqrt(criterion(predictions, targets)).item(),2)\n",
    "    metrics['r'] = round(pearsonr(predictions, targets).item(),3)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def scatterplot(targets, predictions, metrics, ax):\n",
    "    ax.scatter(targets, predictions, zorder=1)\n",
    "    ax.set_xlim(0.0, scale_factor_target)\n",
    "    ax.set_ylim(-0.2*scale_factor_target, 1.0*scale_factor_target)\n",
    "    ax.plot([-0.2*scale_factor_target, 1.0*scale_factor_target], [-0.2*scale_factor_target, 1.0*scale_factor_target], color='grey')\n",
    "    ax.grid()\n",
    "    ax.set_title(f'{str(metrics)} n={len(predictions)}')\n",
    "    ax.set_xlabel('Observed yield (fresh ton/ha)')\n",
    "    ax.set_ylabel('Predicted yield (fresh ton/ha)')\n",
    "\n",
    "    \n",
    "def compute_and_log_stats(predictions, targets, writer, tag, step):\n",
    "    metrics = compute_metrics(predictions, targets)\n",
    "    writer.add_scalar(f'{tag}/rmse', metrics['rmse'], step)\n",
    "    writer.add_scalar(f'{tag}/r', metrics['r'], step)\n",
    "\n",
    "    fig_tb, ax_tb = plt.subplots(1,1)\n",
    "    scatterplot(targets.cpu().detach(), predictions.cpu().detach(), metrics, ax_tb)\n",
    "    writer.add_figure(f'{tag}', fig_tb, step)\n",
    "    writer.flush()\n",
    "    \n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, n_timeseries=6, l_timeseries=213, n_continuous=5, n_categories=32):\n",
    "        super(Model1, self).__init__()\n",
    "        n_hidden_timeseries = 15\n",
    "        n_hidden_continuous = 15\n",
    "        n_embedded = 5\n",
    "        self.cnn_features = nn.Sequential(\n",
    "            nn.Conv1d(n_timeseries, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(3, 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(2, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.cnn_linear = nn.Sequential(\n",
    "            nn.Linear(int(l_timeseries/4), n_hidden_timeseries),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.cnn_scalar = nn.Sequential(\n",
    "            nn.Linear(n_continuous, n_hidden_continuous),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.embed = nn.Embedding(n_categories, n_embedded)\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.BatchNorm1d(n_hidden_timeseries + n_hidden_continuous + n_embedded),\n",
    "            nn.Linear(n_hidden_timeseries + n_hidden_continuous + n_embedded, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(5, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, timeseries, continuous, categories):\n",
    "        x1 = self.cnn_features(timeseries)\n",
    "        x1 = torch.flatten(x1,1)\n",
    "        x1 = self.cnn_linear(x1)\n",
    "\n",
    "        x2 = self.cnn_scalar(continuous)\n",
    "        x3 = torch.squeeze(self.embed(categories), dim=1)\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.combine(x)\n",
    "        return x\n",
    "    \n",
    "        self.avg_timeseries = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=l_timeseries)\n",
    "        )\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, n_timeseries=6, l_timeseries=213, n_continuous=5):\n",
    "        super(Model2, self).__init__()\n",
    "        n_hidden_timeseries = 15\n",
    "        n_hidden_continuous = 15\n",
    "        n_embedded = 5\n",
    "        \n",
    "        self.avg_timeseries = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=l_timeseries)\n",
    "        )\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(n_timeseries + n_continuous, 1),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "            #nn.Linear(n_timeseries + n_continuous, 5),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Linear(5, 1),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, timeseries, continuous, categories):\n",
    "        x1 = self.avg_timeseries(timeseries)\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x = torch.cat((x1, continuous), dim=1)\n",
    "        x = self.combine(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def train(dataloader, model, optimizer, writer = None, epoch = None):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for batch, (inputs_timeseries, inputs_continuous, inputs_categorical, targets, index) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
    "        loss = loss_fn(pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(dataloader, model, writer = None, tag = None, epoch = None):\n",
    "    model.eval()\n",
    "    targets_storage, predictions_storage = [], []\n",
    "    with torch.no_grad():\n",
    "        for (inputs_timeseries, inputs_continuous, inputs_categorical, targets, index) in dataloader:\n",
    "            pred = model(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
    "            predictions_storage.append(pred)\n",
    "            targets_storage.append(targets)\n",
    "        predictions = torch.mul(torch.reshape(torch.cat(predictions_storage),(-1,)), scale_factor_target)\n",
    "        targets = torch.mul(torch.reshape(torch.cat(targets_storage),(-1,)), scale_factor_target)\n",
    "        if writer:\n",
    "            compute_and_log_stats(predictions, targets, writer, tag , epoch)\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e9a94-9546-423f-90da-60e47668978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = DataLoader(datasets['train'], batch_size=128, shuffle=True)\n",
    "inputs_timeseries, inputs_continuous, inputs_categories, targets, indices = next(iter(dataloaders['train'] ))\n",
    "min_days, n_channels_timeseries, n_features_continuous = inputs_timeseries.shape[2], inputs_timeseries.shape[1], inputs_continuous.shape[1]\n",
    "\n",
    "dataloaders['val'] = DataLoader(datasets['test'], batch_size=len(datasets['test']), shuffle=False)\n",
    "dataloaders['test'] = DataLoader(datasets['obs'], batch_size=len(datasets['obs']), shuffle=False)\n",
    "dataloaders['tipstar'] = DataLoader(datasets['tipstar'], batch_size=len(datasets['tipstar']), shuffle=False)\n",
    "\n",
    "tag = 'sigmoid'\n",
    "current_day = datetime.datetime.now().strftime(\"%b-%d\")\n",
    "tb_dir = os.path.join(repodir, f'{tag}-{current_day}')\n",
    "\n",
    "for seed in range(25):\n",
    "    torch.random.manual_seed(seed)\n",
    "    model = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    writer = {}\n",
    "    for setup in ['synthetic']: #'transfer', 'datadriven', , 'tipstar']:\n",
    "        writer[setup] = SummaryWriter(log_dir=os.path.join(tb_dir, f'{setup}-{seed}'))\n",
    "\n",
    "    for epoch in range(15):\n",
    "        if epoch % 5 or epoch < 2:\n",
    "            #test(dataloader_train, model, writer['synthetic'], 'train', epoch)\n",
    "            test(dataloaders['val'], model, writer=writer['synthetic'], tag='val', epoch=epoch)\n",
    "            test(dataloaders['test'], model, writer=writer['synthetic'], tag='test', epoch=epoch)\n",
    "            test(dataloaders['tipstar'], model, writer=writer['synthetic'], tag='test-tipstar', epoch=epoch)\n",
    "        train(dataloaders['train'], model, optimizer, writer['synthetic'], epoch)\n",
    "  \n",
    "    torch.save(model.state_dict(), os.path.join(tb_dir, f'{setup}-{seed}.pth'))\n",
    "    del model\n",
    "    del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a9221-5e0b-445e-8170-e658d4ee8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def kaiming_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            if param.dim() == 3:\n",
    "                param.data.normal_(0, math.sqrt(2) / math.sqrt(param.shape[1]))\n",
    "\n",
    "expids=list(set(datasets['obs'].experimentids))\n",
    "\n",
    "def expid_to_year(expid):\n",
    "    number = ''.join([x for x in expid if x.isdigit()])\n",
    "    return number[0:2]\n",
    "    \n",
    "inputs_timeseries, inputs_continuous, inputs_categories, targets, indices = next(iter(DataLoader(datasets['obs'], batch_size=64, shuffle=True)))\n",
    "min_days, n_channels_timeseries, n_features_continuous = inputs_timeseries.shape[2], inputs_timeseries.shape[1], inputs_continuous.shape[1]\n",
    "\n",
    "tag = 'cv-1'\n",
    "current_day = datetime.datetime.now().strftime(\"%b-%d\")\n",
    "tb_dir = os.path.join(repodir, f'log/{tag}-{current_day}')\n",
    "setup = 'datadriven'\n",
    "#dict_exp = {nr : i for i, nr in enumerate(expids)}\n",
    "def def_value(): return []\n",
    "dict_exp = defaultdict(def_value)\n",
    "for expid in expids:\n",
    "    year = expid_to_year(expid)\n",
    "    dict_exp[year].append(expid)\n",
    "\n",
    "\n",
    "dataset = datasets['obs']\n",
    "num_epochs = 500\n",
    "\n",
    "r_storage, rmse_storage, seeds = [], [], []\n",
    "for seed in range(20):\n",
    "    log_dir = os.path.join(tb_dir, f'{setup}-{seed}')\n",
    "    writer = {}\n",
    "    writer[setup] = SummaryWriter(log_dir=log_dir)    \n",
    "    models = {}\n",
    "    optimizers = {}\n",
    "    for fold, id in enumerate(dict_exp):\n",
    "        torch.random.manual_seed(seed)\n",
    "        model = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
    "        kaiming_init(model)\n",
    "        models[fold] = model\n",
    "        optimizers[fold] = torch.optim.Adam(models[fold].parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        do_log = (epoch % 10 == 0) or (epoch < 15 and epoch % 3 == 0) or (epoch == (num_epochs-1))\n",
    "        if do_log:\n",
    "            predictions_storage, targets_storage = [], []\n",
    "        for fold, id in enumerate(dict_exp):\n",
    "            train_idx = np.where(dataset.experimentids.map(lambda x: expid_to_year(x)) != id)[0]\n",
    "            #train_idx = np.where(dataset.experimentids != id)[0]\n",
    "            train_set = Subset(dataset, train_idx)\n",
    "            train_loader = DataLoader(train_set, batch_size = 128, shuffle = True)\n",
    "            if do_log:\n",
    "                val_idx = np.where(dataset.experimentids.map(lambda x: expid_to_year(x)) == id)[0]\n",
    "                #val_idx = np.where(dataset.experimentids == id)[0]\n",
    "                test_set = Subset(dataset, val_idx)\n",
    "                test_loader = DataLoader(test_set, batch_size=len(val_idx))\n",
    "                predictions_fold, targets_fold = test(test_loader, models[fold])\n",
    "                predictions_storage.append(predictions_fold)\n",
    "                targets_storage.append(targets_fold)\n",
    "            train(train_loader, models[fold], optimizers[fold])\n",
    "           \n",
    "        if do_log:        \n",
    "            predictions = torch.reshape(torch.cat(predictions_storage),(-1,))\n",
    "            targets = torch.reshape(torch.cat(targets_storage),(-1,))\n",
    "            compute_and_log_stats(predictions, targets, writer['datadriven'], 'test' , epoch)\n",
    "            \n",
    "            predictions_train, targets_train = test(train_loader, models[fold])\n",
    "            compute_and_log_stats(predictions_train, targets_train, writer['datadriven'], 'train' , epoch)\n",
    "\n",
    "    output = pd.DataFrame(list(zip(predictions.numpy(), targets.numpy())), columns = ['prediction', 'target'])\n",
    "    output.to_csv(f'{tb_dir}/{setup}-{seed}.csv', index=False)\n",
    "    \n",
    "    metrics = compute_metrics(targets, predictions)\n",
    "    r_storage.append(metrics['r'])\n",
    "    rmse_storage.append(metrics['rmse'])\n",
    "    seeds.append(seed)\n",
    "    output = pd.DataFrame(list(zip(seeds, r_storage, rmse_storage)), columns = ['seed', 'r', 'rmse'])\n",
    "    print(f\"update {tb_dir}/{setup}.txt with {seed} {metrics['r']} {metrics['rmse']}\")\n",
    "    output.to_csv(f'{tb_dir}/{setup}.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab645ca-1eb6-4bba-9728-23373d7d50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_storage, rmse_storage = {}, {}\n",
    "dataloaders = {}\n",
    "for s in ['obs', 'tipstar']:\n",
    "    dataloaders[s] = DataLoader(datasets[s], batch_size=len(datasets[s]), shuffle=False)\n",
    "    r_storage[f'model-{s}'], rmse_storage[f'model-{s}'] = [], []\n",
    "\n",
    "inputs_timeseries, inputs_continuous, inputs_categories, targets, indices = next(iter(DataLoader(datasets['train'], batch_size=64, shuffle=True)))\n",
    "min_days, n_channels_timeseries, n_features_continuous  = inputs_timeseries.shape[2], inputs_timeseries.shape[1], inputs_continuous.shape[1]\n",
    "\n",
    "tag = 'sigmoid-Jun-18'\n",
    "modeldir = os.path.join(repodir,f'{tag}')\n",
    "\n",
    "for i in range(25):\n",
    "    model = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
    "    model_path = os.path.join(modeldir, f'synthetic-{i}.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    for s in ['obs', 'tipstar']:\n",
    "        predictions, targets = test(dataloaders[s], model)\n",
    "        metrics = compute_metrics(predictions, targets)\n",
    "        r_storage[f'model-{s}'].append(metrics['r'])\n",
    "        rmse_storage[f'model-{s}'].append(metrics['rmse'])\n",
    "    \n",
    "    predictions, targets = test(dataloaders['obs'], model)\n",
    "    output = pd.DataFrame(list(zip(predictions.numpy(), targets.numpy())), columns = ['prediction', 'target'])\n",
    "    output.to_csv(os.path.join(modeldir, f'synthetic-{i}.csv'), index=False)\n",
    "\n",
    "tipstar_predictions = pd.read_csv(os.path.join(csvdir,'tipstar-obs.csv.gz'), sep=',')\n",
    "observations = pd.read_csv(os.path.join(csvdir,'response-obs.csv.gz'), sep=',')\n",
    "targets = torch.mul(torch.tensor(observations['x'].values), scale_factor_target)\n",
    "predictions = torch.mul(torch.tensor(tipstar_predictions['x'].values), scale_factor_target)\n",
    "r_storage['tipstar-obs'], rmse_storage['tipstar-obs'] = [] , []\n",
    "nboot = 1000\n",
    "for b in range(nboot):\n",
    "    idx = torch.randint(len(targets), (len(targets),))\n",
    "    targets_boot = targets[idx]\n",
    "    predictions_boot = predictions[idx]\n",
    "    metrics_boot = compute_metrics(targets_boot, predictions_boot)\n",
    "    r_storage['tipstar-obs'].append(metrics_boot['r'])\n",
    "    rmse_storage['tipstar-obs'].append(metrics_boot['rmse'])\n",
    "    \n",
    "cv_data = pd.read_csv(f'{repodir}/log/cv-1-Aug-16/datadriven.txt', sep=',')\n",
    "r_storage['datadriven-obs'] = cv_data['r'].tolist()\n",
    "rmse_storage['datadriven-obs'] = cv_data['rmse'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef67c1a-af78-4b22-af66-f199d4265d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [Line2D([0], [0], color='grey', lw=2),\n",
    "                Line2D([0], [0], color='r', lw=2),\n",
    "                Line2D([0], [0], color='b', lw=2),\n",
    "                Line2D([0], [0], color='m', lw=2)]\n",
    "\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "\n",
    "stats_r, stats_rmse = {}, {}\n",
    "for s in ['model-obs', 'tipstar-obs', 'datadriven-obs']:\n",
    "    stats_r[s] = sigma_clipped_stats(r_storage[s])\n",
    "    stats_rmse[s] = sigma_clipped_stats(rmse_storage[s])\n",
    "\n",
    "colors = {'Basemodel': 'r', 'Transfer Learning': 'b', 'Datadriven': 'm'}\n",
    "#rmse = {'Basemodel': 11.95, 'Transfer Learning': 9.63, 'Datadriven': 9.44}\n",
    "#r = {'Basemodel': 0.61, 'Transfer Learning': 0.57, 'Datadriven': 0.6}\n",
    "    \n",
    "fig, axes = plt.subplots(1, 2)\n",
    "bplot_rmse = axes[0].boxplot([rmse_storage['tipstar-obs'], rmse_storage['model-obs'], rmse_storage['datadriven-obs']])\n",
    "labels=[f\"tipstar\\n ({round(stats_rmse['tipstar-obs'][1],2)} +/- {round(stats_rmse['tipstar-obs'][2],2)})\",\n",
    "        f\"basemodel\\n ({round(stats_rmse['model-obs'][1],2)} +/- {round(stats_rmse['model-obs'][2],2)})\",\n",
    "        f\"datadriven\\n ({round(stats_rmse['datadriven-obs'][1],2)} +/- {round(stats_rmse['datadriven-obs'][2],2)})\"]\n",
    "axes[0].set_title(f'RMSE')\n",
    "axes[0].set_xticklabels(labels, fontsize=6.5)\n",
    "\n",
    "#for k in r.keys():\n",
    "#    axes[0].axhline(rmse[k], c=colors[k], ls=\"--\")   \n",
    "#axes[0].axhline(11.36, c='grey', linestyle='dotted')\n",
    "\n",
    "axes[0].set_ylim(9,20)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "bplot_r = axes[1].boxplot([r_storage['tipstar-obs'], r_storage['model-obs'], r_storage['datadriven-obs']])\n",
    "labels=[f\"tipstar\\n ({round(stats_r['tipstar-obs'][1],3)} +/- {round(stats_r['tipstar-obs'][2],3)})\",\n",
    "        f\"basemodel\\n ({round(stats_r['model-obs'][1],3)} +/- {round(stats_r['model-obs'][2],3)})\",\n",
    "        f\"datadriven\\n ({round(stats_r['datadriven-obs'][1],3)} +/- {round(stats_r['datadriven-obs'][2],3)})\"]\n",
    "\n",
    "axes[1].set_title(f'r')\n",
    "axes[1].set_xticklabels(labels, fontsize=6.5)\n",
    "#for k in r.keys():\n",
    "#    axes[1].axhline(r[k], c=colors[k], ls=\"--\")\n",
    "axes[1].set_ylim(0.1,0.8)\n",
    "\n",
    "bplot_rmse['medians'][1].set(color='r')\n",
    "bplot_rmse['medians'][2].set(color='m')\n",
    "bplot_r['medians'][1].set(color='r')\n",
    "bplot_r['medians'][2].set(color='m')\n",
    "\n",
    "\n",
    "fig.legend(custom_lines, ['y=55.87', 'Basemodel', 'Transfer Learning', 'Datadriven'], loc='upper right', bbox_to_anchor=(1.3, 0.9))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cef6a8-cb93-4442-a4b6-7615d1de1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ds_path = os.path.join(csvdir, f'si-obs.csv')\n",
    "tipstar_data = pd.read_csv(ds_path, sep=',')\n",
    "tipstar_data['site'] = tipstar_data['exp'].map(lambda x: str(x)[0:2].capitalize())#.split('T')[-2])\n",
    "tipstar_data['ex'] = tipstar_data['exp'].map(lambda x: str(x).split('T')[-2].capitalize())\n",
    "tipstar_data['sidedress'] = np.where(((tipstar_data['sidedressdoy'] >0.0) & (tipstar_data['sidedressamount'] >-1.0)), 'True', 'False')\n",
    "tipstar_data['y03'] = np.where(((tipstar_data['year'] ==0.30)), 'True', 'False')\n",
    "\n",
    "expids = ['DRV00', 'DRV96', 'DRV97', 'DRV98', 'DRV99', 'KB009035', 'KB019045', 'KB029055',\n",
    " 'KB039074', 'KB961083', 'KB971106', 'KB981118', 'KB981119', 'KB981120',\n",
    " 'KB981121', 'KB991139', 'KB991140', 'KB999019', 'KP009059', 'KP029112',\n",
    " 'KP039147', 'KP940316', 'KP950000', 'KP960366', 'KP970384', 'KP980407',\n",
    " 'KP980408', 'KP980411', 'KP990436', 'KP990437', 'KP999038', 'Kp980415',\n",
    " 'kb009036', 'kb999020', 'kp009060', 'kp999039']\n",
    "\n",
    "dict_exp = {nr : i for i, nr in enumerate(expids)}\n",
    "   \n",
    "fig, ax = plt.subplots(1,1)    \n",
    "sns.scatterplot(ax=ax, data=tipstar_data, y=\"model\", x=\"tipstar\", hue=(tipstar_data[\"sidedress\"].astype(\"category\")))\n",
    "plt.plot([0, 100000], [0, 100000], linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jAKC3gZWj0L0",
   "metadata": {
    "id": "jAKC3gZWj0L0"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import numpy as np\n",
    "obs_yield = torch.mul(datasets['obs'].tensor_targets, scale_factor_target).numpy()\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plt.hist(obs_yield)\n",
    "plt.show()\n",
    "mean_predictions = np.mean(obs_yield)\n",
    "rmse = np.sqrt(np.mean((mean_predictions-obs_yield)**2))\n",
    "print(mean_predictions, rmse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "metatipstar-no-lightning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
