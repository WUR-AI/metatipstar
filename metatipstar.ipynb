{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233e7390-0242-42c9-8389-e5a3b0dfec1b",
      "metadata": {
        "id": "233e7390-0242-42c9-8389-e5a3b0dfec1b"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "    colab = 'google.colab' in str(get_ipython())\n",
        "    if not colab:\n",
        "        print('This cell is only for Google Colab')\n",
        "        return\n",
        "\n",
        "    !rm -fr /content/metatipstar\n",
        "    !git clone https://yourusername:yourtoken@github.com/BigDataWUR/metatipstar.git\n",
        "    !pip install --quiet \"pytorch-lightning\" \"audtorch\"\n",
        "    inputdir = '/content/metatipstar/csv'\n",
        "setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1",
      "metadata": {
        "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class NitrogenDataset(torch.utils.data.Dataset):\n",
        "    \"\"\" Custom PyTorch dataset \"\"\"\n",
        "\n",
        "    def __init__(self, tensor_timeseries, tensor_scalars, tensor_targets, \n",
        "                 featurenames_timeseries, featurenames_scalars, featurenames_targets):\n",
        "        assert tensor_timeseries.size(dim=1) == len(featurenames_timeseries)\n",
        "        assert tensor_scalars.size(dim=1) == len(featurenames_scalars)\n",
        "        assert tensor_targets.size(dim=1) == len(featurenames_targets)\n",
        "        self.tensor_timeseries = torch.nan_to_num(tensor_timeseries)\n",
        "        self.tensor_scalars = torch.nan_to_num(tensor_scalars)\n",
        "        self.tensor_targets = torch.nan_to_num(tensor_targets)\n",
        "        self.featurenames_timeseries = featurenames_timeseries\n",
        "        self.featurenames_scalars = featurenames_scalars\n",
        "        self.featurenames_targets = featurenames_targets\n",
        "        self.tensor_scalars[:,self.featurenames_scalars.index('harvestdoy')] = 0.0\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Returns the size of the dataset\"\n",
        "        return len(self.tensor_timeseries)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Returns an element from the dataset\"\n",
        "        return self.tensor_timeseries[index], self.tensor_scalars[index], self.tensor_targets[index]\n",
        "\n",
        "feature_variables = ['irradiance','irrigation','mintemp','maxtemp','precipitation',\n",
        "                     'baseN','sidedressdoy','sidedressamount','sowdoy','maxRootDepthDueToSoil',\n",
        "                     'soilprofile_nr_8090','soilprofile_nr_4040','soilprofile_nr_4031','soilprofile_nr_2070','soilprofile_nr_8110',\n",
        "                     'soilprofile_nr_10240','soilprofile_nr_4010','soilprofile_nr_4070','soilprofile_nr_2080','soilprofile_nr_4050',\n",
        "                     'soilprofile_nr_8060','soilprofile_nr_10010','soilprofile_nr_4160','soilprofile_nr_8101','soilprofile_nr_11030',\n",
        "                     'soilprofile_nr_2040','soilprofile_nr_8120','soilprofile_nr_4140','soilprofile_nr_2060','soilprofile_nr_10080',\n",
        "                     'soilprofile_nr_2050','soilprofile_nr_10190','soilprofile_nr_10030','soilprofile_nr_11040','soilprofile_nr_10191',\n",
        "                     'soilprofile_nr_2010','soilprofile_nr_4020','soilprofile_nr_4090','soilprofile_nr_10061','soilprofile_nr_11050',\n",
        "                     'soilprofile_nr_4041','soilprofile_nr_4130',\n",
        "                     'Earliness','harvestdoy']\n",
        "timeseries = ['irradiance','irrigation','mintemp','maxtemp','precipitation']\n",
        "target_variables = ['max_tuberfreshwt']\n",
        "\n",
        "def create_torch_dataset(num_examples, length_time_series, feature_variables, inputdir, tag):\n",
        "    print(f'create dataset {tag} {num_examples}')\n",
        "    set_timeseries = sorted(list(set(feature_variables) & set(timeseries)))\n",
        "    set_scalars = sorted(list(set(feature_variables) - set(set_timeseries)))\n",
        "    tensor_timeseries = torch.zeros([num_examples, len(set_timeseries), length_time_series])\n",
        "    tensor_scalars = torch.zeros([num_examples, len(set_scalars)])\n",
        "    tensor_targets = torch.zeros([num_examples, len(target_variables)])\n",
        "    featurenames_timeseries = []\n",
        "    for i, name in enumerate(set_timeseries):\n",
        "        ds_path = os.path.join(inputdir, f'{name}-{tag}.csv.gz')\n",
        "        v_data = pd.read_csv(ds_path, sep=',')\n",
        "        print(f'load {ds_path} {v_data.shape}')\n",
        "        tensor_timeseries[:,i,:] = torch.tensor(v_data.values)\n",
        "        featurenames_timeseries.append(name)\n",
        "    ds_path = os.path.join(inputdir, f'si-{tag}.csv.gz')\n",
        "    scalar_data = pd.read_csv(ds_path, sep=',')\n",
        "    print(f'load {ds_path} {scalar_data.shape}')\n",
        "    scalar_data = scalar_data[set_scalars]\n",
        "    featurenames_scalars = list(scalar_data.columns)\n",
        "    tensor_scalars[:,:] = torch.tensor(scalar_data.values)\n",
        "   \n",
        "    ds_path = os.path.join(inputdir, f'response-{tag}.csv.gz')\n",
        "    target_data = pd.read_csv(ds_path, sep=',')\n",
        "    print(f'load {ds_path} {target_data.shape}')\n",
        "    target_data.columns = target_variables\n",
        "    target_data = target_data[target_variables]\n",
        "    featurenames_targets = list(target_data.columns)\n",
        "    tensor_targets[:,:] = torch.tensor(target_data.values)\n",
        "    dataset = NitrogenDataset(tensor_timeseries, tensor_scalars, tensor_targets, featurenames_timeseries, featurenames_scalars, featurenames_targets)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f",
      "metadata": {
        "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f"
      },
      "outputs": [],
      "source": [
        "#sets = ['train', 'val', 'test', 'obs']\n",
        "sets = ['train', 'obs']\n",
        "inputdir = '/content/metatipstar/csv'\n",
        "datasets = {}\n",
        "for s in sets:\n",
        "    ds_path = os.path.join(inputdir,f'irradiance-{s}.csv.gz')\n",
        "    num_examples, length_timeseries = pd.read_csv(ds_path, sep=',').shape\n",
        "    dataset = create_torch_dataset(num_examples, length_timeseries, feature_variables, inputdir=inputdir, tag=s)\n",
        "    datasets[s] = dataset    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab32c8c-7a05-4aa4-b180-053278a469db",
      "metadata": {
        "id": "3ab32c8c-7a05-4aa4-b180-053278a469db"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "fig, axes = plt.subplots(len(sets),1, sharex=True, figsize=(15,20)) #, subplot_kw=dict(box_aspect=1)\n",
        "for i,s in enumerate(sets):\n",
        "    ax = axes[i]\n",
        "    x=np.tile(range(datasets[s].tensor_timeseries.shape[2]),(5,1)).T\n",
        "    sample=randrange(datasets[s].tensor_timeseries.shape[0])\n",
        "    y=datasets[s].tensor_timeseries[sample,:,:].T\n",
        "    ax.step(x, y, where='post',label=datasets[s].featurenames_timeseries)\n",
        "    ax.set_title(f'{s} (i={sample})')\n",
        "fig.legend(datasets['train'].featurenames_timeseries) #, bbox_to_anchor=(0.9,0.9), loc=\"upper right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe",
      "metadata": {
        "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "features = datasets['train'].featurenames_scalars\n",
        "features = [f for f in features if not 'soil' in f]\n",
        " \n",
        "fig, axes = plt.subplots(len(features),1, figsize=(20,20))\n",
        "for i,featurename in enumerate(features):\n",
        "    print(f'{i}/{len(features)} feature: {featurename}')\n",
        "    ax_left=axes[i]\n",
        "    df_list = []\n",
        "    f = datasets['train'].featurenames_scalars.index(featurename)\n",
        "    for s in sets:\n",
        "        y=datasets[s].tensor_scalars[:,f].detach()\n",
        "        df = pd.DataFrame(data=y,columns=[featurename])\n",
        "        df['dataset'] = s\n",
        "        df_list.append(df)\n",
        "    df_long = pd.concat(df_list, sort=False,ignore_index=True)\n",
        "    sns.histplot(data=df_long, x=featurename, hue='dataset', stat=\"probability\", common_norm=False, ax=ax_left, legend=False, multiple='dodge') #, clip=(0,1))\n",
        "    ax_left.set_xlabel('')\n",
        "    ax_left.set_title(f'{featurename}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d2ed0a-c080-4273-ae69-88a8d4595a43",
      "metadata": {
        "id": "f5d2ed0a-c080-4273-ae69-88a8d4595a43"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from audtorch.metrics.functional import pearsonr\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model1(pl.LightningModule):\n",
        "    def __init__(self, n_input_channels=6, n_input_samples=213, n_scalars=5 ):\n",
        "        super(Model1, self).__init__()\n",
        "        n_hidden_timeseries = 15\n",
        "        self.test_results = {}\n",
        "        self.cnn_features = nn.Sequential(\n",
        "            nn.Conv1d(n_input_channels, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(3, 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(2, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.cnn_linear = nn.Sequential(\n",
        "            nn.Linear(int(n_input_samples/4), n_hidden_timeseries)\n",
        "        )\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.BatchNorm1d(n_hidden_timeseries + n_scalars),\n",
        "            nn.Linear(n_hidden_timeseries + n_scalars, 5),\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, timeseries, scalars):\n",
        "        x1 = self.cnn_features(timeseries)\n",
        "        x1 = torch.flatten(x1,1)\n",
        "        x1 = self.cnn_linear(x1)\n",
        "\n",
        "        x2 = scalars\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = self.combine(x)\n",
        "        return x\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_scalars, targets = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_scalars)\n",
        "        loss = F.mse_loss(x, targets)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return {\"loss\": loss, \"predictions\": x, \"targets\": targets}\n",
        "\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in training_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in training_step_outputs]),(-1,))\n",
        "        r = pearsonr(predictions, targets)\n",
        "        print(f'R-train: {r.detach()}')\n",
        "        self.log(\"R-train\", r)\n",
        "        \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_scalars, targets = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_scalars)\n",
        "        loss = F.mse_loss(x, targets)\n",
        "        self.log(\"validation_loss\", loss)\n",
        "        return {\"loss\": loss, \"predictions\": x, \"targets\": targets}\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in validation_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in validation_step_outputs]),(-1,))\n",
        "        r = pearsonr(predictions, targets)\n",
        "        print(f'R-val: {r.detach()}')\n",
        "        self.log(\"R-val\", r)    \n",
        "\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_scalars, targets = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_scalars)\n",
        "        return_values = {\n",
        "            \"predictions\": x,  # list of len batch \n",
        "            \"targets\": targets,  # list of len batch \n",
        "        } \n",
        "        return return_values\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return_values = self.predict_step(batch, batch_idx)\n",
        "        return return_values\n",
        "\n",
        "    def test_epoch_end(self, test_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in test_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in test_step_outputs]),(-1,))\n",
        "        return_values = {\n",
        "            \"predictions\": predictions,  # list of len batch \n",
        "            \"groundtruth\": targets,  # list of len batch \n",
        "        }\n",
        "        self.test_results = return_values\n",
        "        return return_values\n",
        "    \n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "dataloader_train = DataLoader(datasets['train'], batch_size=4, shuffle=True)\n",
        "inputs_timeseries, inputs_scalars, targets = next(iter(dataloader_train))\n",
        "min_days = inputs_timeseries.shape[2]\n",
        "n_channels_timeseries = inputs_timeseries.shape[1]\n",
        "n_features_scalars = inputs_scalars.shape[1]\n",
        "model = Model1(n_input_channels=n_channels_timeseries, n_input_samples=min_days, n_scalars=n_features_scalars)\n",
        "output = model(inputs_timeseries, inputs_scalars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70680023-c3ab-4367-9f1a-dc21954c5ac6",
      "metadata": {
        "id": "70680023-c3ab-4367-9f1a-dc21954c5ac6"
      },
      "outputs": [],
      "source": [
        "def divisors(n):\n",
        "    import math\n",
        "    divs = [1]\n",
        "    for i in range(2,int(math.sqrt(n))+1):\n",
        "        if n%i == 0:\n",
        "            divs.extend([i,n/i])\n",
        "    divs.extend([n])\n",
        "    return list(set(divs))\n",
        "\n",
        "def find_batch_size(n, k=500):\n",
        "    d = divisors(n)\n",
        "    b = max(filter(lambda i: i < k, d))\n",
        "    return b\n",
        "\n",
        "# init model\n",
        "model1 = Model1(n_input_channels=n_channels_timeseries, n_input_samples=min_days, n_scalars=n_features_scalars)\n",
        "\n",
        "n_samples_train = len(DataLoader(datasets['train']))\n",
        "batch_size_train = find_batch_size(len(DataLoader(datasets['train'])))\n",
        "print(f'train: n={n_samples_train} batch_size={batch_size_train}')\n",
        "dataloader_train = DataLoader(datasets['train'], batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "n_samples_obs = len(DataLoader(datasets['obs']))\n",
        "batch_size_obs = find_batch_size(len(DataLoader(datasets['obs'])))\n",
        "print(f'observations: n={n_samples_obs} batch_size={batch_size_obs}')\n",
        "dataloader_obs = DataLoader(datasets['obs'], batch_size=batch_size_obs, shuffle=False)\n",
        "\n",
        "log_dir = inputdir = '/content/metatipstar/log'\n",
        "\n",
        "if not os.path.isdir(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $log_dir\n",
        "\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "logger = pl_loggers.TensorBoardLogger(log_dir, name=\"initial-model\")\n",
        "trainer = pl.Trainer(devices=1, max_epochs=50, logger=logger, accelerator=\"auto\") \n",
        "trainer.fit(model=model1, train_dataloaders=dataloader_train, val_dataloaders=dataloader_obs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "metatipstar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}