{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXrYtDFXjcfk"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "    colab = 'google.colab' in str(get_ipython())\n",
        "    if not colab:\n",
        "        print('Please run in Google Colab')\n",
        "        return\n",
        "\n",
        "    !rm -fr /content/metatipstar\n",
        "    !git clone https://your-username:your-token@github.com/BigDataWUR/metatipstar.git\n",
        "    !pip install --quiet \"pytorch-lightning\" \"audtorch\"\n",
        "    inputdir = '/content/metatipstar/csv'\n",
        "setup()"
      ],
      "id": "hXrYtDFXjcfk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "soilnr = [2010, 2040, 2050, 2060, 2070, 2080, 4010, 4020, 4031, \n",
        " 4040, 4041, 4050, 4070, 4090, 4130, 4140, 4160, 8060,\n",
        " 8090, 8101, 8110, 8120, 10010, 10030, 10061, 10080, \n",
        " 10190, 10191, 10240, 11030, 11040, 11050]\n",
        "dict_soilnr = {nr : i for i, nr in enumerate(soilnr)}\n",
        "\n",
        "feature_variables = ['irradiance','irrigation','mintemp','maxtemp','precipitation',\n",
        "                     'baseN','sidedressdoy','sidedressamount','sowdoy','maxRootDepthDueToSoil', 'Earliness',\n",
        "                     'soilprofile']\n",
        "target_variables = ['max_tuberfreshwt']\n",
        "timeseries = ['irradiance','irrigation','mintemp','maxtemp','precipitation']\n",
        "categorical = ['soilprofile']\n",
        "\n",
        "def kaiming_init(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        #print(f'{name} {param.requires_grad} {param.dim()}')\n",
        "        if name.endswith(\".bias\"):\n",
        "            param.data.fill_(0)\n",
        "        #elif name.startswith(\"layers.0\"):  # The first layer does not have ReLU applied on its input\n",
        "        #    param.data.normal_(0, 1 / math.sqrt(param.shape[1]))\n",
        "        else:\n",
        "            if param.dim() == 3:\n",
        "              param.data.normal_(0, math.sqrt(2) / math.sqrt(param.shape[1]))\n",
        "\n",
        "\n",
        "def divisors(n):\n",
        "    import math\n",
        "    divs = [1]\n",
        "    for i in range(2,int(math.sqrt(n))+1):\n",
        "        if n%i == 0:\n",
        "            divs.extend([i,n/i])\n",
        "    divs.extend([n])\n",
        "    return list(set(divs))\n",
        "\n",
        "def find_batch_size(n, k=500):\n",
        "    d = divisors(n)\n",
        "    b = max(filter(lambda i: i < k, d))\n",
        "    return int(b)\n",
        "\n",
        "\n",
        "class NitrogenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tensor_timeseries, tensor_continuous, tensor_categorical, tensor_targets, \n",
        "                 featurenames_timeseries, featurenames_continuous, featurenames_categorical, featurenames_targets):\n",
        "        assert tensor_timeseries.size(dim=1) == len(featurenames_timeseries)\n",
        "        assert tensor_continuous.size(dim=1) == len(featurenames_continuous)\n",
        "        assert tensor_categorical.size(dim=1) == len(featurenames_categorical)\n",
        "        assert tensor_targets.size(dim=1) == len(featurenames_targets)\n",
        "        self.tensor_timeseries = torch.nan_to_num(tensor_timeseries)\n",
        "        self.tensor_continuous = torch.nan_to_num(tensor_continuous)\n",
        "        self.tensor_categorical = torch.nan_to_num(tensor_categorical)\n",
        "        self.tensor_targets = torch.nan_to_num(tensor_targets)\n",
        "        self.featurenames_timeseries = featurenames_timeseries\n",
        "        self.featurenames_continuous = featurenames_continuous\n",
        "        self.featurenames_categorical = featurenames_categorical\n",
        "        self.featurenames_targets = featurenames_targets\n",
        "        #quick hack to correct flaws in 'sidedressdoy'\n",
        "        self.tensor_continuous[:,self.featurenames_continuous.index('sidedressdoy')] = 0.0\n",
        "        self.experimentids = []\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Returns the size of the dataset\"\n",
        "        return len(self.tensor_timeseries)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Returns an element from the dataset\"\n",
        "        return self.tensor_timeseries[index], self.tensor_continuous[index], self.tensor_categorical[index], self.tensor_targets[index], index\n",
        "\n",
        "\n",
        "def create_torch_dataset(num_examples, length_time_series, feature_variables, inputdir, tag):\n",
        "    print(f'create dataset {tag} {num_examples}')\n",
        "    set_timeseries = sorted(list(set(feature_variables) & set(timeseries)))\n",
        "    set_scalars = sorted(list(set(feature_variables) - set(set_timeseries)))\n",
        "    set_continuous = sorted(list(set(set_scalars) - set(categorical)))\n",
        "    tensor_timeseries = torch.zeros([num_examples, len(set_timeseries), length_time_series])\n",
        "    tensor_continuous = torch.zeros([num_examples, len(set_continuous)])\n",
        "    tensor_categorical = torch.zeros([num_examples, len(categorical)],dtype=int)\n",
        "    tensor_targets = torch.zeros([num_examples, len(target_variables)])\n",
        "    featurenames_timeseries = []\n",
        "    for i, name in enumerate(set_timeseries):\n",
        "        ds_path = os.path.join(inputdir, f'{name}-{tag}.csv.gz')\n",
        "        v_data = pd.read_csv(ds_path, sep=',')\n",
        "        print(f'load {ds_path} {v_data.shape}')\n",
        "        tensor_timeseries[:,i,:] = torch.tensor(v_data.values)\n",
        "        featurenames_timeseries.append(name)\n",
        "    ds_path = os.path.join(inputdir, f'si-{tag}.csv.gz')\n",
        "    scalar_data = pd.read_csv(ds_path, sep=',')\n",
        "    print(f'load {ds_path} {scalar_data.shape}')\n",
        "    set_continuous = scalar_data[set_continuous]\n",
        "    featurenames_continuous = list(set_continuous.columns)\n",
        "    tensor_continuous[:,:] = torch.tensor(set_continuous.values)\n",
        "\n",
        "    for i, c in enumerate(categorical):\n",
        "      df_nr = scalar_data.filter(regex=c).idxmax(axis=1).map(lambda x: int(str(x).split('_')[-1]))\n",
        "      df_nr = df_nr.map(lambda x : dict_soilnr[x])\n",
        "      tensor_categorical[:,i] = torch.tensor(df_nr.values.astype(int))\n",
        "   \n",
        "    ds_path = os.path.join(inputdir, f'response-{tag}.csv.gz')\n",
        "    target_data = pd.read_csv(ds_path, sep=',')\n",
        "    print(f'load {ds_path} {target_data.shape}')\n",
        "    target_data.columns = target_variables\n",
        "    target_data = target_data[target_variables]\n",
        "    featurenames_targets = list(target_data.columns)\n",
        "    tensor_targets[:,:] = torch.tensor(target_data.values)\n",
        "    dataset = NitrogenDataset(tensor_timeseries, tensor_continuous, tensor_categorical, tensor_targets, featurenames_timeseries, featurenames_continuous, categorical, featurenames_targets)\n",
        "    if 'exp' in scalar_data:  \n",
        "      dataset.experimentids = scalar_data['exp'].map(lambda x: str(x).split('T')[-2])\n",
        "    return dataset"
      ],
      "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f"
      },
      "outputs": [],
      "source": [
        "sets = ['train', 'val', 'test', 'obs']\n",
        "sets = ['train', 'obs']\n",
        "inputdir = '/content/metatipstar/csv'\n",
        "datasets = {}\n",
        "for s in sets:\n",
        "    ds_path = os.path.join(inputdir,f'irradiance-{s}.csv.gz')\n",
        "    num_examples, length_timeseries = pd.read_csv(ds_path, sep=',').shape\n",
        "    dataset = create_torch_dataset(num_examples, length_timeseries, feature_variables, inputdir=inputdir, tag=s)\n",
        "    datasets[s] = dataset"
      ],
      "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ab32c8c-7a05-4aa4-b180-053278a469db"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "fig, axes = plt.subplots(len(sets),1, sharex=True, figsize=(15,20)) #, subplot_kw=dict(box_aspect=1)\n",
        "for i,s in enumerate(sets):\n",
        "    ax = axes[i]\n",
        "    x=np.tile(range(datasets[s].tensor_timeseries.shape[2]),(5,1)).T\n",
        "    sample=randrange(datasets[s].tensor_timeseries.shape[0])\n",
        "    y=datasets[s].tensor_timeseries[sample,:,:].T\n",
        "    ax.step(x, y, where='post',label=datasets[s].featurenames_timeseries)\n",
        "    ax.set_title(f'{s} (i={sample})')\n",
        "    ax.set_ylim((0,1))\n",
        "fig.legend(datasets['train'].featurenames_timeseries) #, bbox_to_anchor=(0.9,0.9), loc=\"upper right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "id": "3ab32c8c-7a05-4aa4-b180-053278a469db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOIb-HEW0BSh"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "nvar = datasets[s].tensor_timeseries.shape[1]\n",
        "nx = datasets[s].tensor_timeseries.shape[2]\n",
        "fig, axes = plt.subplots(nvar,1, sharex=True, figsize=(15,20))\n",
        "for v,v_name in enumerate(datasets['train'].featurenames_timeseries):  \n",
        "  ax = axes[v]\n",
        "  plot_data = np.zeros((nx,len(sets)))\n",
        "  for i,s in enumerate(sets):\n",
        "    y = datasets[s].tensor_timeseries[:,v,:].numpy().squeeze()\n",
        "    y = np.mean(y,axis=0).T\n",
        "    plot_data[:,i] = y\n",
        "  x = np.tile(range(nx),(len(sets),1)).T\n",
        "  ax.step(x, plot_data, where='post',label=s,zorder=0)\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.grid()\n",
        "  ax.set_title(f'{v_name}')\n",
        "fig.legend(sets) #, bbox_to_anchor=(0.9,0.9), loc=\"upper right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "id": "jOIb-HEW0BSh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "features = datasets['train'].featurenames_continuous + datasets['train'].featurenames_categorical + datasets['train'].featurenames_targets\n",
        " \n",
        "fig, axes = plt.subplots(len(features),1, figsize=(20,20))\n",
        "for i,featurename in enumerate(features):\n",
        "    print(f'{i}/{len(features)} feature: {featurename}')\n",
        "    ax_left=axes[i]\n",
        "    df_list = []\n",
        "    for s in sets:\n",
        "      if featurename in categorical:\n",
        "        f = datasets['train'].featurenames_categorical.index(featurename)\n",
        "        y = datasets[s].tensor_categorical[:,f].detach()/len(dict_soilnr)\n",
        "      elif featurename in target_variables:\n",
        "        f = datasets['train'].featurenames_targets.index(featurename)\n",
        "        y = datasets[s].tensor_targets[:,f].detach()\n",
        "      else:\n",
        "        f = datasets['train'].featurenames_continuous.index(featurename)\n",
        "        y = datasets[s].tensor_continuous[:,f].detach()\n",
        "      df = pd.DataFrame(data=y,columns=[featurename])\n",
        "      df['dataset'] = s\n",
        "      df_list.append(df)\n",
        "    df_long = pd.concat(df_list, sort=False,ignore_index=True)\n",
        "    sns.histplot(data=df_long, x=featurename, hue='dataset', stat=\"probability\", common_norm=False, ax=ax_left, legend=False, multiple='dodge')\n",
        "    ax_left.set_xlabel('')\n",
        "    ax_left.set_title(f'{featurename}')\n",
        "plt.show()"
      ],
      "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5d2ed0a-c080-4273-ae69-88a8d4595a43"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from audtorch.metrics.functional import pearsonr\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scale_factor_target = 107827.207557798 / 1000.0\n",
        "\n",
        "class Model1(pl.LightningModule):\n",
        "    def __init__(self, n_timeseries=6, l_timeseries=213, n_continuous=5, n_categories=32, do_log=True):\n",
        "        super(Model1, self).__init__()\n",
        "        n_hidden_timeseries = 15\n",
        "        n_hidden_continuous = 15\n",
        "        n_embedded = 5\n",
        "        self.do_log = do_log\n",
        "        self.cnn_features = nn.Sequential(\n",
        "            nn.Conv1d(n_timeseries, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(3, 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(2, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.cnn_linear = nn.Sequential(\n",
        "            nn.Linear(int(l_timeseries/4), n_hidden_timeseries),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.cnn_scalar = nn.Sequential(\n",
        "            nn.Linear(n_continuous, n_hidden_continuous),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.embed = nn.Embedding(n_categories, n_embedded)\n",
        "\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.BatchNorm1d(n_hidden_timeseries + n_hidden_continuous + n_embedded),\n",
        "            nn.Linear(n_hidden_timeseries + n_hidden_continuous + n_embedded, 5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, timeseries, continuous, categories):\n",
        "        x1 = self.cnn_features(timeseries)\n",
        "        x1 = torch.flatten(x1,1)\n",
        "        x1 = self.cnn_linear(x1)\n",
        "\n",
        "        x2 = self.cnn_scalar(continuous)\n",
        "        x3 = torch.squeeze(self.embed(categories), dim=1)\n",
        "        x = torch.cat((x1, x2, x3), dim=1)\n",
        "        x = self.combine(x)\n",
        "        return x\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_continuous, inputs_categorical, targets, index = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
        "        loss = F.mse_loss(x, targets)\n",
        "        if self.do_log: \n",
        "          self.log(\"train_loss\", loss.item())\n",
        "        return {\"loss\": loss, \"predictions\": x, \"targets\": targets}\n",
        "\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in training_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in training_step_outputs]),(-1,))\n",
        "        r = pearsonr(predictions, targets)\n",
        "        criterion = nn.MSELoss()\n",
        "        predictions_rescaled = torch.mul(predictions, scale_factor_target)\n",
        "        targets_rescaled = torch.mul(targets, scale_factor_target) \n",
        "        rmse = torch.sqrt(criterion(predictions_rescaled, targets_rescaled))\n",
        "        if self.do_log:\n",
        "          self.log(\"R-train\", r)\n",
        "          self.log(\"rmse-train\", rmse)\n",
        "        return {\"r\": r.item(), \"rmse\": rmse.item(), \"targets\": targets_rescaled.detach(), \"predictions\": predictions_rescaled.detach()}  \n",
        "        \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_continuous, inputs_categorical, targets, index = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
        "        loss = F.mse_loss(x, targets)\n",
        "        if self.do_log:\n",
        "          self.log(\"validation_loss\", loss)\n",
        "        return {\"loss\": loss, \"predictions\": x, \"targets\": targets}\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in validation_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in validation_step_outputs]),(-1,))\n",
        "        predictions_rescaled = torch.mul(predictions, scale_factor_target)\n",
        "        targets_rescaled = torch.mul(targets, scale_factor_target) \n",
        "        r = pearsonr(predictions, targets)\n",
        "        if False:\n",
        "          criterion = nn.MSELoss()\n",
        "          rmse = torch.sqrt(criterion(predictions_rescaled, targets_rescaled))\n",
        "          fig, ax = plt.subplots(1,1)\n",
        "          plt.scatter(targets_rescaled.cpu(), predictions_rescaled.cpu(), zorder=1)\n",
        "          plt.xlim(0.0, scale_factor_target)\n",
        "          plt.ylim(0.0, scale_factor_target)\n",
        "          ax.plot([0, 1], [0, 1], transform=ax.transAxes, color='grey')\n",
        "          plt.grid()\n",
        "          plt.title(f'epoch {self.current_epoch} R={r.item():.3f} RMSE={rmse:.2f} n={len(predictions)}')       \n",
        "          self.logger.experiment.add_figure(\"validation\", fig, self.global_step)\n",
        "        if self.do_log:\n",
        "          self.log(\"R-val\", r)\n",
        "        return_values = {\n",
        "            \"predictions\": predictions_rescaled.cpu(),  # list of len batch \n",
        "            \"targets\": targets_rescaled.cpu(),  # list of len batch \n",
        "        }\n",
        "\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_continuous, inputs_categorical, targets, index = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
        "        return_values = {\n",
        "            \"predictions\": x,  # list of len batch \n",
        "            \"targets\": targets,  # list of len batch \n",
        "        }\n",
        "        return return_values\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return_values = self.predict_step(batch, batch_idx)\n",
        "        return return_values\n",
        "\n",
        "    def test_epoch_end(self, test_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in test_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in test_step_outputs]),(-1,))\n",
        "        return_values = {\n",
        "            \"predictions\": predictions,  # list of len batch \n",
        "            \"groundtruth\": targets,  # list of len batch \n",
        "        }\n",
        "        return return_values\n",
        "    \n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "# some sanity checks\n",
        "dataloader_train = DataLoader(datasets['train'], batch_size=4, shuffle=True)\n",
        "inputs_timeseries, inputs_continuous, inputs_categories, targets, indices = next(iter(dataloader_train))\n",
        "min_days = inputs_timeseries.shape[2]\n",
        "n_channels_timeseries = inputs_timeseries.shape[1]\n",
        "n_features_continuous = inputs_continuous.shape[1]\n",
        "\n",
        "model = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "output = model(inputs_timeseries, inputs_continuous, inputs_categories)"
      ],
      "id": "f5d2ed0a-c080-4273-ae69-88a8d4595a43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70680023-c3ab-4367-9f1a-dc21954c5ac6"
      },
      "outputs": [],
      "source": [
        "model1 = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "\n",
        "dataloaders = {}\n",
        "for s in sets:\n",
        "  n_samples = len(DataLoader(datasets[s]))\n",
        "  batch_size = find_batch_size(len(DataLoader(datasets[s])))\n",
        "  print(f'{s}: n={n_samples} batch_size={batch_size}')\n",
        "  dataloaders[s] = DataLoader(datasets[s], batch_size=batch_size, shuffle=(s == \"train\"))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "log_dir = '/content/drive/MyDrive/WUR/metatipstar/log'\n",
        "!mkdir -p $log_dir \n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $log_dir\n",
        "\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "logger = pl_loggers.TensorBoardLogger(log_dir, name=\"synthetic-model\")\n",
        "trainer = pl.Trainer(devices=1, max_epochs=1, logger=logger, accelerator=\"auto\") \n",
        "trainer.fit(model=model1, train_dataloaders=dataloaders['train'], val_dataloaders=dataloaders['obs'])\n",
        "\n",
        "#model_path = os.path.join(log_dir,f'synthetic-model.ckpt')\n",
        "#trainer.save_checkpoint(model_path)"
      ],
      "id": "70680023-c3ab-4367-9f1a-dc21954c5ac6"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import SubsetRandomSampler, Subset, Dataset, DataLoader\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.loops import Loop\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
        "\n",
        "class TrainingEpochLoop(Loop):\n",
        "\n",
        "    def __init__(self, model, optimizer, dataloader):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.dataloader = dataloader\n",
        "        self.batch_idx = 0\n",
        "        self._results = []\n",
        "\n",
        "    @property\n",
        "    def done(self):\n",
        "        return self.batch_idx >= len(self.dataloader)\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.dataloader_iter = iter(self.dataloader)\n",
        "\n",
        "    def advance(self, *args, **kwargs) -> None:\n",
        "        batch = next(self.dataloader_iter)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.model.training_step(batch, self.batch_idx)\n",
        "        loss['loss'].backward()\n",
        "        self._results.append(loss)\n",
        "        self.optimizer.step()\n",
        "    def on_run_end(self):\n",
        "        results = self.model.training_epoch_end(self._results)\n",
        "        return results\n",
        "\n",
        "\n",
        "def scatterplot(targets, predictions, metrics={}, ax=None):\n",
        "  ax.scatter(targets, predictions, zorder=1)\n",
        "  ax.set_xlim(0.0, scale_factor_target)\n",
        "  ax.set_ylim(-0.2*scale_factor_target, scale_factor_target)\n",
        "  ax.plot([-0.2*scale_factor_target, scale_factor_target], [-0.2*scale_factor_target, scale_factor_target], color='grey')\n",
        "  ax.grid()\n",
        "  ax.set_title(f'{str(metrics)} n={len(predictions)}')\n",
        "  ax.set_xlabel('Observed yield (fresh ton/ha)')\n",
        "  ax.set_ylabel('Predicted yield (fresh ton/ha)')\n",
        "\n",
        "def compute_metrics(targets, predictions):\n",
        "  metrics = {}\n",
        "  criterion = nn.MSELoss()\n",
        "  metrics['rmse'] = round(torch.sqrt(criterion(predictions, targets)).item(),2)\n",
        "  metrics['r'] = round(pearsonr(predictions, targets).item(),3)\n",
        "  return metrics\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "log_dir = '/content/drive/MyDrive/WUR/metatipstar/log-try'\n",
        "!mkdir -p $log_dir \n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(log_dir=f'{log_dir}/9-June')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $log_dir\n",
        "\n",
        "num_epochs = 4000\n",
        "model_path = os.path.join('/content/metatipstar/log',f'synthetic-model.ckpt')\n",
        "model_synthetic = Model1.load_from_checkpoint(model_path, n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "model_synthetic.do_log = False\n",
        "\n",
        "dataset = datasets['obs']\n",
        "\n",
        "test_loader = DataLoader(dataset, batch_size=find_batch_size(len(dataset)))\n",
        "trainer = pl.Trainer(logger=False, accelerator=\"auto\", enable_model_summary=False, enable_progress_bar=False)\n",
        "results_synthetic = trainer.predict(model_synthetic, test_loader)\n",
        "predictions = torch.reshape(results_synthetic[0]['predictions'],(-1,))\n",
        "targets = torch.reshape(results_synthetic[0]['targets'],(-1,))\n",
        "predictions = torch.mul(predictions, scale_factor_target)\n",
        "targets = torch.mul(targets, scale_factor_target)\n",
        "metrics_synthetic = compute_metrics(targets, predictions)\n",
        "\n",
        "expids = ['DRV00', 'DRV96', 'DRV97', 'DRV98', 'DRV99', 'KB009035', 'KB019045', 'KB029055',\n",
        " 'KB039074', 'KB961083', 'KB971106', 'KB981118', 'KB981119', 'KB981120',\n",
        " 'KB981121', 'KB991139', 'KB991140', 'KB999019', 'KP009059', 'KP029112',\n",
        " 'KP039147', 'KP940316', 'KP950000', 'KP960366', 'KP970384', 'KP980407',\n",
        " 'KP980408', 'KP980411', 'KP990436', 'KP990437', 'KP999038', 'Kp980415',\n",
        " 'kb009036', 'kb999020', 'kp009060', 'kp999039']\n",
        "\n",
        "dict_exp = {nr : i for i, nr in enumerate(expids)}\n",
        "\n",
        "untrained_model = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "untrained_model.do_log = False\n",
        "\n",
        "models_transfer = {}\n",
        "optimizers_transfer = {}\n",
        "models = {}\n",
        "optimizers = {}\n",
        "for fold, id in enumerate(dict_exp):\n",
        "  models[fold] = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr), do_log=False)\n",
        "  torch.random.manual_seed(42)\n",
        "  kaiming_init(models[fold])\n",
        "  optimizers[fold] = models[fold].configure_optimizers()\n",
        "  model_transfer = Model1.load_from_checkpoint(model_path, n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr), do_log=False)\n",
        "  model_transfer.freeze()\n",
        "  for param in model_transfer.combine.parameters(): param.requires_grad = True  \n",
        "  models_transfer[fold] = model_transfer\n",
        "  optimizers_transfer[fold] = models_transfer[fold].configure_optimizers()\n",
        "\n",
        "\n",
        "#fig_train, axes_train = plt.subplots(len(expids), num_epochs, sharex = True, sharey = True, figsize=(35,100))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  do_log = (epoch % 25 == 0)\n",
        "  if do_log:\n",
        "    predictions_storage, targets_storage = [], []\n",
        "    predictions_transfer_storage, targets_transfer_storage = [], []\n",
        "\n",
        "  for fold, id in enumerate(dict_exp):\n",
        "\n",
        "    if do_log:\n",
        "      val_idx = np.where(dataset.experimentids == id)[0]\n",
        "      test_set = Subset(dataset, val_idx)\n",
        "      test_loader = DataLoader(test_set, batch_size=find_batch_size(len(val_idx)))\n",
        "    \n",
        "      trainer = pl.Trainer(logger=False, accelerator=\"auto\", enable_model_summary=False, enable_progress_bar=False)\n",
        "      test_results = trainer.predict(models[fold], test_loader)\n",
        "      preds = torch.mul(test_results[0]['predictions'].cpu(), scale_factor_target)\n",
        "      targs = torch.mul(test_results[0]['targets'].cpu(), scale_factor_target)\n",
        "      predictions_storage.append(preds)\n",
        "      targets_storage.append(targs)\n",
        "\n",
        "      trainer_transfer = pl.Trainer(logger=False, accelerator=\"auto\", enable_model_summary=False, enable_progress_bar=False)\n",
        "      test_results_transfer = trainer_transfer.predict(models_transfer[fold], test_loader)\n",
        "      preds_transfer = torch.mul(test_results_transfer[0]['predictions'].cpu(), scale_factor_target)\n",
        "      targs_transfer = torch.mul(test_results_transfer[0]['targets'].cpu(), scale_factor_target)\n",
        "      predictions_transfer_storage.append(preds_transfer)\n",
        "      targets_transfer_storage.append(targs_transfer)\n",
        "\n",
        "    train_idx = np.where(dataset.experimentids != id)[0]\n",
        "    train_set = Subset(dataset, train_idx)\n",
        "    train_loader = DataLoader(train_set, batch_size = find_batch_size(len(train_idx)), shuffle = True)\n",
        "    train_metrics = TrainingEpochLoop(models[fold], optimizers[fold], train_loader).run()\n",
        "    train_metrics_transfer = TrainingEpochLoop(models_transfer[fold], optimizers_transfer[fold], train_loader).run()\n",
        "\n",
        "    if do_log and fold == 0:\n",
        "      metrics = compute_metrics(train_metrics['targets'], train_metrics['predictions'])\n",
        "      metrics_transfer = compute_metrics(train_metrics_transfer['targets'], train_metrics_transfer['predictions'])\n",
        "      writer.add_scalars(\"rmse-train\", {'rmse':metrics['rmse'], 'rmse-transfer':metrics_transfer['rmse'], 'rmse-synthetic':metrics_synthetic['rmse']}, epoch)\n",
        "      writer.add_scalars(\"r-train\", {'r':metrics['r'], 'r-transfer':metrics_transfer['r'], 'r-synthetic':metrics_synthetic['r']}, epoch)\n",
        "\n",
        "      fig_tb, ax_tb = plt.subplots(1,1)\n",
        "      scatterplot(train_metrics['targets'], train_metrics['predictions'], metrics, ax_tb)\n",
        "      writer.add_figure(\"train\", fig_tb, epoch)\n",
        "\n",
        "      fig_tb, ax_tb = plt.subplots(1,1)\n",
        "      scatterplot(train_metrics_transfer['targets'], train_metrics_transfer['predictions'], metrics_transfer, ax_tb)\n",
        "      writer.add_figure(\"train-transfer\", fig_tb, epoch)    \n",
        "\n",
        "\n",
        "  if do_log == False: continue \n",
        "  predictions = torch.reshape(torch.cat(predictions_storage),(-1,))\n",
        "  targets = torch.reshape(torch.cat(targets_storage),(-1,))\n",
        "  metrics = compute_metrics(targets, predictions)\n",
        "  metrics['epoch'] = epoch\n",
        "\n",
        "  predictions_transfer = torch.reshape(torch.cat(predictions_transfer_storage),(-1,))\n",
        "  targets_transfer = torch.reshape(torch.cat(targets_transfer_storage),(-1,))\n",
        "  metrics_transfer = compute_metrics(targets_transfer, predictions_transfer)\n",
        "  metrics_transfer['epoch'] = epoch\n",
        "\n",
        "  writer.add_scalars(\"rmse-test\", {'rmse':metrics['rmse'], 'rmse-transfer':metrics_transfer['rmse'], 'rmse-synthetic':metrics_synthetic['rmse']}, epoch)\n",
        "  writer.add_scalars(\"r-test\", {'r':metrics['r'], 'r-transfer':metrics_transfer['r'], 'r-synthetic':metrics_synthetic['r']}, epoch)\n",
        "\n",
        "  fig_tb, ax_tb = plt.subplots(1,1)\n",
        "  scatterplot(targets, predictions, metrics, ax_tb)\n",
        "  writer.add_figure(\"validation\", fig_tb, epoch)\n",
        "\n",
        "  fig_tb, ax_tb = plt.subplots(1,1)\n",
        "  scatterplot(targets_transfer, predictions_transfer, metrics_transfer, ax_tb)\n",
        "  writer.add_figure(\"validation-transfer\", fig_tb, epoch)\n",
        "  writer.flush()\n",
        "\n",
        "#fig_train.show()\n"
      ],
      "metadata": {
        "id": "ZvtlQkUVSyuL"
      },
      "id": "ZvtlQkUVSyuL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "import numpy as np\n",
        "obs_yield = torch.mul(datasets['obs'].tensor_targets, scale_factor_target).numpy()\n",
        "fig, ax = plt.subplots(1,1)\n",
        "plt.hist(obs_yield)\n",
        "plt.show()\n",
        "mean_predictions = np.mean(obs_yield)\n",
        "rmse = np.sqrt(np.mean((mean_predictions-obs_yield)**2))\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "jAKC3gZWj0L0"
      },
      "id": "jAKC3gZWj0L0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "import logging\n",
        "import numpy as np\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
        "\n",
        "dataset = datasets['obs']\n",
        "model1 = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr), do_log=False)\n",
        "\n",
        "expids = ['DRV00', 'DRV96', 'DRV97', 'DRV98', 'DRV99', 'KB009035', 'KB019045', 'KB029055',\n",
        " 'KB039074', 'KB961083', 'KB971106', 'KB981118', 'KB981119', 'KB981120',\n",
        " 'KB981121', 'KB991139', 'KB991140', 'KB999019', 'KP009059', 'KP029112',\n",
        " 'KP039147', 'KP940316', 'KP950000', 'KP960366', 'KP970384', 'KP980407',\n",
        " 'KP980408', 'KP980411', 'KP990436', 'KP990437', 'KP999038', 'Kp980415',\n",
        " 'kb009036', 'kb999020', 'kp009060', 'kp999039']\n",
        "\n",
        "dict_exp = {nr : i for i, nr in enumerate(expids)}\n",
        "\n",
        "results_datamodel = {}\n",
        "results_synthetic = {}\n",
        "\n",
        "for fold,id in enumerate(dict_exp):\n",
        "  train_idx = np.where(dataset.experimentids != id)[0]\n",
        "  val_idx = np.where(dataset.experimentids ==id)[0]\n",
        "  print(f'{fold} {id} {len(train_idx)} {len(val_idx)}')\n",
        "  logger = pl_loggers.TensorBoardLogger(log_dir, name=f'data-model-{fold}')\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  test_sampler = SubsetRandomSampler(val_idx)\n",
        "  train_loader = DataLoader(dataset, batch_size=find_batch_size(len(train_idx)), sampler=train_sampler)\n",
        "  test_loader = DataLoader(dataset, batch_size=find_batch_size(len(val_idx)), sampler=test_sampler)\n",
        "  trainer = pl.Trainer(devices=1, max_epochs=500, logger=logger, accelerator=\"auto\", enable_model_summary=False, enable_progress_bar=False)\n",
        "  model_cv = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr), do_log=False)\n",
        "  trainer.fit(model=model_cv, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
        "  results_datamodel[fold] = trainer.predict(model_cv, test_loader)\n",
        "  model_path = os.path.join(log_dir,f'synthetic-model.ckpt')\n",
        "  model_synthetic = Model1.load_from_checkpoint(model_path, n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "  results_synthetic[fold] = trainer.predict(model_synthetic, test_loader)\n",
        "\n",
        "\n",
        "  del model_cv"
      ],
      "metadata": {
        "id": "TMv-vWNTkfcc"
      },
      "id": "TMv-vWNTkfcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "def scatterplot(targets, predictions, metrics={}, ax=None):\n",
        "  ax.scatter(targets, predictions, zorder=1)\n",
        "  ax.set_xlim(0.0, scale_factor_target)\n",
        "  ax.set_ylim(0.0, scale_factor_target)\n",
        "  ax.plot([0, 1], [0, 1], transform=ax.transAxes, color='grey')\n",
        "  ax.grid()\n",
        "  ax.set_title(f'{str(metrics)} n={len(predictions)}')\n",
        "  ax.set_xlabel('Observed yield (fresh ton/ha)')\n",
        "  ax.set_ylabel('Predicted yield (fresh ton/ha)')\n",
        "\n",
        "def compute_metrics(targets, predictions):\n",
        "  metrics = {}\n",
        "  criterion = nn.MSELoss()\n",
        "  metrics['rmse'] = round(torch.sqrt(criterion(predictions, targets)).item(),2)\n",
        "  metrics['r'] = round(pearsonr(predictions, targets).item(),3)\n",
        "  return metrics\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "for i, results in enumerate ([results_datamodel, results_synthetic]):\n",
        "  ax = axes[i]\n",
        "  results = {key: value for key, value in results.items() if key <30}\n",
        "  predictions = torch.reshape(torch.cat([x[0]['predictions'] for x in results.values()]),(-1,))\n",
        "  targets = torch.reshape(torch.cat([x[0]['targets'] for x in results.values()]),(-1,))\n",
        "  #predictions = torch.reshape(results_synthetic[0]['predictions'],(-1,))\n",
        "  #targets = torch.reshape(results_synthetic[0]['targets'],(-1,))\n",
        "  predictions_rescaled = torch.mul(predictions, scale_factor_target)\n",
        "  targets_rescaled = torch.mul(targets, scale_factor_target)\n",
        "  metrics = compute_metrics(targets_rescaled, predictions_rescaled)\n",
        "  scatterplot(targets_rescaled, predictions_rescaled, metrics, ax)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xmA0B4o-pPY4"
      },
      "id": "xmA0B4o-pPY4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "metatipstar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}