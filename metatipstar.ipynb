{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233e7390-0242-42c9-8389-e5a3b0dfec1b",
      "metadata": {
        "id": "233e7390-0242-42c9-8389-e5a3b0dfec1b"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "    colab = 'google.colab' in str(get_ipython())\n",
        "    if not colab:\n",
        "        print('Please run in Google Colab')\n",
        "        return\n",
        "\n",
        "    !rm -fr /content/metatipstar\n",
        "    !git clone https://your-username:your-keyR@github.com/BigDataWUR/metatipstar.git\n",
        "    !pip install --quiet \"pytorch-lightning\" \"audtorch\"\n",
        "    inputdir = '/content/metatipstar/csv'\n",
        "setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1",
      "metadata": {
        "id": "9fe06ee4-29ac-42ca-8702-f57d9fcdd5a1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "soilnr = [2010, 2040, 2050, 2060, 2070, 2080, 4010, 4020, 4031, \n",
        " 4040, 4041, 4050, 4070, 4090, 4130, 4140, 4160, 8060,\n",
        " 8090, 8101, 8110, 8120, 10010, 10030, 10061, 10080, \n",
        " 10190, 10191, 10240, 11030, 11040, 11050]\n",
        "dict_soilnr = {nr : i for i, nr in enumerate(soilnr)}\n",
        "\n",
        "feature_variables = ['irradiance','irrigation','mintemp','maxtemp','precipitation',\n",
        "                     'baseN','sidedressdoy','sidedressamount','sowdoy','maxRootDepthDueToSoil', 'Earliness',\n",
        "                     'soilprofile']\n",
        "target_variables = ['max_tuberfreshwt']\n",
        "timeseries = ['irradiance','irrigation','mintemp','maxtemp','precipitation']\n",
        "categorical = ['soilprofile']\n",
        "\n",
        "\n",
        "class NitrogenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tensor_timeseries, tensor_continuous, tensor_categorical, tensor_targets, \n",
        "                 featurenames_timeseries, featurenames_continuous, featurenames_categorical, featurenames_targets):\n",
        "        assert tensor_timeseries.size(dim=1) == len(featurenames_timeseries)\n",
        "        assert tensor_continuous.size(dim=1) == len(featurenames_continuous)\n",
        "        assert tensor_categorical.size(dim=1) == len(featurenames_categorical)\n",
        "        assert tensor_targets.size(dim=1) == len(featurenames_targets)\n",
        "        self.tensor_timeseries = torch.nan_to_num(tensor_timeseries)\n",
        "        self.tensor_continuous = torch.nan_to_num(tensor_continuous)\n",
        "        self.tensor_categorical = torch.nan_to_num(tensor_categorical)\n",
        "        self.tensor_targets = torch.nan_to_num(tensor_targets)\n",
        "        self.featurenames_timeseries = featurenames_timeseries\n",
        "        self.featurenames_continuous = featurenames_continuous\n",
        "        self.featurenames_categorical = featurenames_categorical\n",
        "        self.featurenames_targets = featurenames_targets\n",
        "        #quick hack to correct flaws in 'sidedressdoy'\n",
        "        self.tensor_continuous[:,self.featurenames_continuous.index('sidedressdoy')] = 0.0\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Returns the size of the dataset\"\n",
        "        return len(self.tensor_timeseries)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Returns an element from the dataset\"\n",
        "        return self.tensor_timeseries[index], self.tensor_continuous[index], self.tensor_categorical[index], self.tensor_targets[index]\n",
        "\n",
        "\n",
        "def create_torch_dataset(num_examples, length_time_series, feature_variables, inputdir, tag):\n",
        "    print(f'create dataset {tag} {num_examples}')\n",
        "    set_timeseries = sorted(list(set(feature_variables) & set(timeseries)))\n",
        "    set_scalars = sorted(list(set(feature_variables) - set(set_timeseries)))\n",
        "    set_continuous = sorted(list(set(set_scalars) - set(categorical)))\n",
        "    tensor_timeseries = torch.zeros([num_examples, len(set_timeseries), length_time_series])\n",
        "    tensor_continuous = torch.zeros([num_examples, len(set_continuous)])\n",
        "    tensor_categorical = torch.zeros([num_examples, len(categorical)],dtype=int)\n",
        "    tensor_targets = torch.zeros([num_examples, len(target_variables)])\n",
        "    featurenames_timeseries = []\n",
        "    for i, name in enumerate(set_timeseries):\n",
        "        ds_path = os.path.join(inputdir, f'{name}-{tag}.csv.gz')\n",
        "        v_data = pd.read_csv(ds_path, sep=',')\n",
        "        print(f'load {ds_path} {v_data.shape}')\n",
        "        tensor_timeseries[:,i,:] = torch.tensor(v_data.values)\n",
        "        featurenames_timeseries.append(name)\n",
        "    ds_path = os.path.join(inputdir, f'si-{tag}.csv.gz')\n",
        "    scalar_data = pd.read_csv(ds_path, sep=',')\n",
        "    print(f'load {ds_path} {scalar_data.shape}')\n",
        "    set_continuous = scalar_data[set_continuous]\n",
        "    featurenames_continuous = list(set_continuous.columns)\n",
        "    tensor_continuous[:,:] = torch.tensor(set_continuous.values)\n",
        "\n",
        "    for i, c in enumerate(categorical):\n",
        "      df_nr = scalar_data.filter(regex=c).idxmax(axis=1).map(lambda x: int(str(x).split('_')[-1]))\n",
        "      df_nr = df_nr.map(lambda x : dict_soilnr[x])\n",
        "      tensor_categorical[:,i] = torch.tensor(df_nr.values.astype(int))\n",
        "   \n",
        "    ds_path = os.path.join(inputdir, f'response-{tag}.csv.gz')\n",
        "    target_data = pd.read_csv(ds_path, sep=',')\n",
        "    print(f'load {ds_path} {target_data.shape}')\n",
        "    target_data.columns = target_variables\n",
        "    target_data = target_data[target_variables]\n",
        "    featurenames_targets = list(target_data.columns)\n",
        "    tensor_targets[:,:] = torch.tensor(target_data.values)\n",
        "    dataset = NitrogenDataset(tensor_timeseries, tensor_continuous, tensor_categorical, tensor_targets, featurenames_timeseries, featurenames_continuous, categorical, featurenames_targets)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f",
      "metadata": {
        "id": "5434ee0f-02b7-490f-913f-2a3b0c051e1f"
      },
      "outputs": [],
      "source": [
        "#sets = ['train', 'val', 'test', 'obs']\n",
        "sets = ['train', 'obs']\n",
        "inputdir = '/content/metatipstar/csv'\n",
        "datasets = {}\n",
        "for s in sets:\n",
        "    ds_path = os.path.join(inputdir,f'irradiance-{s}.csv.gz')\n",
        "    num_examples, length_timeseries = pd.read_csv(ds_path, sep=',').shape\n",
        "    dataset = create_torch_dataset(num_examples, length_timeseries, feature_variables, inputdir=inputdir, tag=s)\n",
        "    datasets[s] = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab32c8c-7a05-4aa4-b180-053278a469db",
      "metadata": {
        "id": "3ab32c8c-7a05-4aa4-b180-053278a469db"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "fig, axes = plt.subplots(len(sets),1, sharex=True, figsize=(15,20)) #, subplot_kw=dict(box_aspect=1)\n",
        "for i,s in enumerate(sets):\n",
        "    ax = axes[i]\n",
        "    x=np.tile(range(datasets[s].tensor_timeseries.shape[2]),(5,1)).T\n",
        "    sample=randrange(datasets[s].tensor_timeseries.shape[0])\n",
        "    y=datasets[s].tensor_timeseries[sample,:,:].T\n",
        "    ax.step(x, y, where='post',label=datasets[s].featurenames_timeseries)\n",
        "    ax.set_title(f'{s} (i={sample})')\n",
        "fig.legend(datasets['train'].featurenames_timeseries) #, bbox_to_anchor=(0.9,0.9), loc=\"upper right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe",
      "metadata": {
        "id": "408f8725-63d5-4bc3-9c27-a0eb96c64afe"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "features = datasets['train'].featurenames_continuous + datasets['train'].featurenames_categorical\n",
        " \n",
        "fig, axes = plt.subplots(len(features),1, figsize=(20,20))\n",
        "for i,featurename in enumerate(features):\n",
        "    print(f'{i}/{len(features)} feature: {featurename}')\n",
        "    ax_left=axes[i]\n",
        "    df_list = []\n",
        "    for s in sets:\n",
        "      if featurename in categorical:\n",
        "        f = datasets['train'].featurenames_categorical.index(featurename)\n",
        "        y = datasets[s].tensor_categorical[:,f].detach()/len(dict_soilnr)\n",
        "      else:\n",
        "        f = datasets['train'].featurenames_continuous.index(featurename)\n",
        "        y = datasets[s].tensor_continuous[:,f].detach()\n",
        "      df = pd.DataFrame(data=y,columns=[featurename])\n",
        "      df['dataset'] = s\n",
        "      df_list.append(df)\n",
        "    df_long = pd.concat(df_list, sort=False,ignore_index=True)\n",
        "    sns.histplot(data=df_long, x=featurename, hue='dataset', stat=\"probability\", common_norm=False, ax=ax_left, legend=False, multiple='dodge')\n",
        "    ax_left.set_xlabel('')\n",
        "    ax_left.set_title(f'{featurename}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d2ed0a-c080-4273-ae69-88a8d4595a43",
      "metadata": {
        "id": "f5d2ed0a-c080-4273-ae69-88a8d4595a43"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from audtorch.metrics.functional import pearsonr\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scale_factor_target = 107827.207557798 / 1000.0\n",
        "\n",
        "class Model1(pl.LightningModule):\n",
        "    def __init__(self, n_timeseries=6, l_timeseries=213, n_continuous=5, n_categories=32):\n",
        "        super(Model1, self).__init__()\n",
        "        n_hidden_timeseries = 15\n",
        "        n_hidden_continuous = 15\n",
        "        n_embedded = 10\n",
        "        self.test_results = {}\n",
        "        self.cnn_features = nn.Sequential(\n",
        "            nn.Conv1d(n_timeseries, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(3, 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(2, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.cnn_linear = nn.Sequential(\n",
        "            nn.Linear(int(l_timeseries/4), n_hidden_timeseries),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.cnn_scalar = nn.Sequential(\n",
        "            nn.Linear(n_continuous, n_hidden_continuous),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.embed = nn.Embedding(n_categories, n_embedded)\n",
        "\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.BatchNorm1d(n_hidden_timeseries + n_hidden_continuous + n_embedded),\n",
        "            nn.Linear(n_hidden_timeseries + n_hidden_continuous + n_embedded, 5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, timeseries, continuous, categories):\n",
        "        x1 = self.cnn_features(timeseries)\n",
        "        x1 = torch.flatten(x1,1)\n",
        "        x1 = self.cnn_linear(x1)\n",
        "\n",
        "        x2 = self.cnn_scalar(continuous)\n",
        "        x3 = torch.squeeze(self.embed(categories))\n",
        "        x = torch.cat((x1, x2, x3), dim=1)\n",
        "        x = self.combine(x)\n",
        "        return x\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_continuous, inputs_categorical, targets = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
        "        loss = F.mse_loss(x, targets)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return {\"loss\": loss, \"predictions\": x, \"targets\": targets}\n",
        "\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in training_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in training_step_outputs]),(-1,))\n",
        "        r = pearsonr(predictions, targets)\n",
        "        criterion = nn.MSELoss()\n",
        "        predictions_rescaled = torch.mul(predictions, scale_factor_target)\n",
        "        targets_rescaled = torch.mul(targets, scale_factor_target) \n",
        "        rmse = torch.sqrt(criterion(predictions_rescaled, targets_rescaled))\n",
        "        self.log(\"R-train\", r)\n",
        "        self.log(\"rmse-train\", rmse)\n",
        "        \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_continuous, inputs_categorical, targets = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_continuous, inputs_categorical)\n",
        "        loss = F.mse_loss(x, targets)\n",
        "        self.log(\"validation_loss\", loss)\n",
        "        return {\"loss\": loss, \"predictions\": x, \"targets\": targets}\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in validation_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in validation_step_outputs]),(-1,))\n",
        "        predictions_rescaled = torch.mul(predictions, scale_factor_target)\n",
        "        targets_rescaled = torch.mul(targets, scale_factor_target) \n",
        "        r = pearsonr(predictions, targets)\n",
        "        criterion = nn.MSELoss()\n",
        "        rmse = torch.sqrt(criterion(predictions_rescaled, targets_rescaled))\n",
        "        fig, ax = plt.subplots(1,1)\n",
        "        plt.scatter(targets_rescaled.cpu(), predictions_rescaled.cpu(), zorder=1)\n",
        "        plt.xlim(0.0, scale_factor_target)\n",
        "        plt.ylim(0.0, scale_factor_target)\n",
        "        ax.plot([0, 1], [0, 1], transform=ax.transAxes, color='grey')\n",
        "        plt.grid()\n",
        "        plt.title(f'epoch {self.current_epoch} R={r.item():.3f} RMSE={rmse:.2f} n={len(predictions)}')\n",
        "        print(f'epoch {self.current_epoch} R={r.item():.3f} RMSE={rmse:.2f}')\n",
        "        self.logger.experiment.add_figure(\"validation\", fig, self.global_step)\n",
        "        self.log(\"R-val\", r)    \n",
        "\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        inputs_timeseries, inputs_scalars, targets = batch\n",
        "        x = self.forward(inputs_timeseries, inputs_scalars)\n",
        "        return_values = {\n",
        "            \"predictions\": x,  # list of len batch \n",
        "            \"targets\": targets,  # list of len batch \n",
        "        } \n",
        "        return return_values\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return_values = self.predict_step(batch, batch_idx)\n",
        "        return return_values\n",
        "\n",
        "    def test_epoch_end(self, test_step_outputs):\n",
        "        predictions = torch.reshape(torch.stack([x['predictions'] for x in test_step_outputs]),(-1,))\n",
        "        targets = torch.reshape(torch.stack([x['targets'] for x in test_step_outputs]),(-1,))\n",
        "        return_values = {\n",
        "            \"predictions\": predictions,  # list of len batch \n",
        "            \"groundtruth\": targets,  # list of len batch \n",
        "        }\n",
        "        self.test_results = return_values\n",
        "        return return_values\n",
        "    \n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "# some sanity checks\n",
        "dataloader_train = DataLoader(datasets['train'], batch_size=4, shuffle=True)\n",
        "inputs_timeseries, inputs_continuous, inputs_categories, targets = next(iter(dataloader_train))\n",
        "min_days = inputs_timeseries.shape[2]\n",
        "n_channels_timeseries = inputs_timeseries.shape[1]\n",
        "n_features_continuous = inputs_continuous.shape[1]\n",
        "\n",
        "model = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "output = model(inputs_timeseries, inputs_continuous, inputs_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70680023-c3ab-4367-9f1a-dc21954c5ac6",
      "metadata": {
        "id": "70680023-c3ab-4367-9f1a-dc21954c5ac6"
      },
      "outputs": [],
      "source": [
        "def divisors(n):\n",
        "    import math\n",
        "    divs = [1]\n",
        "    for i in range(2,int(math.sqrt(n))+1):\n",
        "        if n%i == 0:\n",
        "            divs.extend([i,n/i])\n",
        "    divs.extend([n])\n",
        "    return list(set(divs))\n",
        "\n",
        "def find_batch_size(n, k=500):\n",
        "    d = divisors(n)\n",
        "    b = max(filter(lambda i: i < k, d))\n",
        "    return b\n",
        "\n",
        "\n",
        "model1 = Model1(n_timeseries=n_channels_timeseries, l_timeseries=min_days, n_continuous=n_features_continuous, n_categories=len(dict_soilnr))\n",
        "\n",
        "dataloaders = {}\n",
        "\n",
        "for s in sets:\n",
        "  n_samples = len(DataLoader(datasets[s]))\n",
        "  batch_size = find_batch_size(len(DataLoader(datasets[s])))\n",
        "  print(f'{s}: n={n_samples} batch_size={batch_size}')\n",
        "  dataloaders[s] = DataLoader(datasets[s], batch_size=batch_size, shuffle=(s == \"train\"))\n",
        "\n",
        "log_dir = inputdir = '/content/metatipstar/log'\n",
        "\n",
        "if not os.path.isdir(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $log_dir\n",
        "\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "logger = pl_loggers.TensorBoardLogger(log_dir, name=\"initial-model\")\n",
        "trainer = pl.Trainer(devices=1, max_epochs=50, logger=logger, accelerator=\"auto\") \n",
        "trainer.fit(model=model1, train_dataloaders=dataloaders['train'], val_dataloaders=dataloaders['obs'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X95xZamXerTf"
      },
      "id": "X95xZamXerTf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "metatipstar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}